{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c532e03241a2f466a57f4ebdb3b92c9b",
     "grade": false,
     "grade_id": "cell-11fe0ac8a0e8c08a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ECMM422 Machine Learning\n",
    "## Course Assessment 1\n",
    "\n",
    "\n",
    "This course assessment (CA1) represents 40% of the overall module assessment.\n",
    "\n",
    "This is an individual exercise and your attention is drawn to the College and University guidelines on collaboration and plagiarism, which are available from the College website.\n",
    "\n",
    "\n",
    "**Note:**\n",
    "1. do not change the name of this notebook, i.e. the notebook file has to be named: ca1.ipynb\n",
    "2. do not remove/delete any cell\n",
    "3. do not add any cell (you can work on a draft notebook and only copy the function implementations here)\n",
    "4. do not add you name or student code in the notebook or in the file name\n",
    "\n",
    "**Evaluation criteria:**\n",
    "\n",
    "Each question asks for one or more functions to be implemented. \n",
    "\n",
    "Each question is awarded a number of marks. \n",
    "\n",
    "A (hidden) unit test is going to evaluate if all desired properties of the required function(s) are met. \n",
    "\n",
    "If the test passes all the associated marks are awarded, if it fails 0 marks are awarded. The large number of questions and sub-questions allows a fine grading. \n",
    "\n",
    "\n",
    "**Efficiency:** There is a cap of a few minutes on the execution of each cell and unit test. Make sure your code is not terribly inefficient (for example having a cell run for hours, e.g. using nested loops rather than NumPy functions that can work directly on  arrays), otherwise the execution of the cell/unit test will be interrupted and considered a failure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19a2cc433c02a6f192c81cede1f87272",
     "grade": false,
     "grade_id": "cell-b7361becee158bc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Notes:\n",
    "\n",
    "We call *rank* the number of indices required to get individual elements of an array. A matrix requires two indices (row, column), and has thus rank 2, a vector requires one index and has rank 1, a scalar does not require any index and has rank 0. The components that make up rank are called *axes* (plural of axis). The dimension is how many elements are in a particular axis. A *shape* is a tuple whose length is the rank and elements are the dimension of each axis.\n",
    "\n",
    "In the rest of the notebook, the term `data matrix` refers to a rank two numpy array where instances are encoded as rows, e.g. a data matrix with 100 rows and 4 columns is to be interpreted as a collection of 100 instances (vectors) each of dimension four.\n",
    "\n",
    "In the rest of the notebook, the term `vector` refers to a rank one numpy array. When the term `distance` is used we mean the Euclidean distance. \n",
    "\n",
    "The functions you are required to write need to take in input and return as output such objects (i.e. not python lists).\n",
    "\n",
    "---\n",
    "\n",
    "When a required function can be implemented directly by a library function it is intended that the candidate should write her own implementation of the function.\n",
    "\n",
    "---\n",
    "\n",
    "Do not assume that the implementations provided in the Workshops exercises contain no mistakes. You should write and are ultimately responsible for the code that you submit in this Assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:05:59.733591Z",
     "start_time": "2023-02-24T12:05:59.341150Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dfc583d591117627e814c28389f7622",
     "grade": false,
     "grade_id": "cell-9e614a36d5268220",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d44cc563188300d4674776c37f058e45",
     "grade": false,
     "grade_id": "cell-41308c8fadb8d614",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1  [marks 6]\n",
    "\n",
    "a) Make a function `data_matrix = make_data_classification(mean, std, n_centres, inner_std, n_samples, random_seed=42)` to create a data matrix according to the following rules:\n",
    "1. `mean` is a n-dimensional vector (say [1,1], but the function should allow vectors of any dimension)\n",
    "2. `n_centres` is the number of centres (say 3) \n",
    "3. `std` is the standard deviation (say 1)\n",
    "4. the centres are sampled from a Normal distribution with mean `mean` and standard deviation `std`\n",
    "5. from each centre sample `n_samples` from a Normal distribution with the centre as the mean and standard deviation `inner_std`\n",
    "so if `mean=[1,1]` `n_centres=3` and `n_samples=10` then the data matrix will be a 30 rows x 2 columns numpy array.\n",
    "\n",
    "b) Make a function `data_matrix, targets = make_data_regression(mean, std, n_centres, inner_std, n_samples, random_seed=42)` to create a data matrix  and a target vector according to the following rules:\n",
    "1. the data matrix is constructed in the same way as in `make_data_classification`\n",
    "2. the targets are the Euclidean distance between the sample and the centre of the generating Normal distribution\n",
    "\n",
    "See Question 3 for a graphical example of the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:05:59.743740Z",
     "start_time": "2023-02-24T12:05:59.736511Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae5712ffbdbc5ffb8119d10c6e0cd274",
     "grade": false,
     "grade_id": "cell-49b50402b19eacea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_data_classification(mean, std, n_centres, inner_std, n_samples, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    centres = np.random.normal(mean, std, size=(n_centres, len(mean)))\n",
    "    \n",
    "    data_matrix = np.concatenate([np.random.normal(centre, inner_std, size=(n_samples, len(mean)))\n",
    "                          for centre in centres])\n",
    "    \n",
    "    return data_matrix\n",
    "\n",
    "def make_data_regression(mean, std, n_centres, inner_std, n_samples, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    centres = np.random.normal(mean, std, size=(n_centres, len(mean)))\n",
    "    \n",
    "    data_matrix = np.empty((n_samples*n_centres, len(mean)))\n",
    "    \n",
    "    for j in range(n_centres):\n",
    "        start_idx = j * n_samples\n",
    "        end_idx = start_idx + n_samples\n",
    "        data_matrix[start_idx:end_idx] = np.random.normal(centres[j], inner_std, size=(n_samples, len(mean)))\n",
    "        \n",
    "    targets = np.empty(n_samples * n_centres)\n",
    "    idx = 0\n",
    "    for j in range(n_centres):\n",
    "        for i in range(n_samples):\n",
    "            targets[idx] = np.linalg.norm(data_matrix[idx] - centres[j])\n",
    "            idx += 1\n",
    "            \n",
    "    return data_matrix, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.427477Z",
     "start_time": "2023-02-24T12:05:59.746225Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "436a06a7c59628ba72548fe773c18388",
     "grade": true,
     "grade_id": "cell-fbf0bfd58ace420a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.434498Z",
     "start_time": "2023-02-24T12:06:00.430108Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f1550f44b5e91700b7015dccd169b2e",
     "grade": true,
     "grade_id": "cell-633aa20749b2a6b8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51ea1d2b15f7b5bded0ade05df9821cc",
     "grade": false,
     "grade_id": "cell-59fc19051b0b0921",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2  [marks 2]\n",
    "\n",
    "a) Make a function `data_matrix, targets = get_dataset_classification(n_samples, std, inner_std)` to create a data matrix and a target vector for a binary classification problem according to the following rules:\n",
    "- the instances from the positive class are generated according to the same rules provided for `make_data_classification`; so are the instances from the negative class \n",
    "- the number of samples for the postive and negative class are balanced\n",
    "- instances from the positive class have as mean the vector [10,10] and those from the negative class, vector [-10,-10]\n",
    "- the number of centres is fixed to 3\n",
    "- the random seed is fixed to 42\n",
    "- `n_samples` indicates the total number of instances finally available in the output `data_matrix`\n",
    "\n",
    "b) Make a function `data_matrix, targets = get_dataset_regression(n_samples, std, inner_std)` to create a data matrix according to the following rules:\n",
    "- the instances are generated according to the same rules provided for `make_data_regression`\n",
    "- the targets are generated according to the same rules provided for `make_data_regression`\n",
    "- instances have as mean the vector [10,10]\n",
    "- the number of centres is fixed to 3\n",
    "- the random seed is fixed to 42\n",
    "- `n_samples` indicates the total number of instances finally available in the output `data_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.440711Z",
     "start_time": "2023-02-24T12:06:00.435979Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a76bbf84050e15f32ffc49e1437e390",
     "grade": false,
     "grade_id": "cell-bff4bb4a60f18d71",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_classification(n_samples, std, inner_std):\n",
    "    random_seed=42\n",
    "    \n",
    "    n_samples_per_class = n_samples // 2\n",
    "    n_centres = 3\n",
    "    \n",
    "    # Generate data for positive class\n",
    "    pos_mean = [10, 10]\n",
    "    pos_data = make_data_classification(pos_mean, std, n_centres=n_centres, inner_std=inner_std, n_samples=n_samples_per_class, random_seed=random_seed)\n",
    "    pos_targets = np.ones(n_samples_per_class * n_centres)\n",
    "    \n",
    "    # Generate data for negative class\n",
    "    neg_mean = [-10, -10]\n",
    "    neg_data = make_data_classification(neg_mean, std, n_centres=n_centres, inner_std=inner_std, n_samples=n_samples_per_class, random_seed=random_seed)\n",
    "    neg_targets = np.zeros(n_samples_per_class * n_centres)\n",
    "    \n",
    "    data_matrix = np.concatenate([pos_data, neg_data])\n",
    "    targets = np.concatenate([pos_targets, neg_targets])\n",
    "    \n",
    "    return data_matrix, targets\n",
    "\n",
    "def get_dataset_regression(n_samples, std, inner_std):\n",
    "    random_seed=42\n",
    "    \n",
    "    # Generate data\n",
    "    mean = [10, 10]\n",
    "    data_matrix, targets = make_data_regression(mean, std, n_centres=3, inner_std=inner_std, n_samples=n_samples, random_seed=random_seed)\n",
    "    \n",
    "    return data_matrix, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.449052Z",
     "start_time": "2023-02-24T12:06:00.442853Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f5999d4363224881986e5f1e0e78632",
     "grade": true,
     "grade_id": "cell-315a764eecd939ce",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d15ce68d8df4f9234316e57318087534",
     "grade": false,
     "grade_id": "cell-a9f40b54d4638aae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 3  [marks 1]\n",
    "\n",
    "Make a function `plot(X,y)` to display the scatter plot of a data matrix of two dimensional instances using the array `y` to assign the colour to the instances.\n",
    "\n",
    "\n",
    "When running \n",
    "\n",
    "```python\n",
    "X, y = get_dataset_regression(n_samples=600, std=30, inner_std=5)\n",
    "plot(X,y)\n",
    "```\n",
    "you should get something like\n",
    "\n",
    "<img src='plot3.png' width=300>\n",
    "\n",
    "and when running\n",
    "\n",
    "```python\n",
    "X, y = get_dataset_classification(n_samples=600, std=30, inner_std=5)\n",
    "plot(X,y)\n",
    "```\n",
    "you should get something like\n",
    "\n",
    "<img src='plot3b.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.456361Z",
     "start_time": "2023-02-24T12:06:00.453084Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "650ac49d731f15d497caf36d43c2f8a9",
     "grade": false,
     "grade_id": "cell-c03ec4203ec30067",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(X,y):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.623961Z",
     "start_time": "2023-02-24T12:06:00.460559Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba4bc39cbbc1f2c95cdba9d7fc966b02",
     "grade": true,
     "grade_id": "cell-8ce0d987b11efe97",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bc5ec9680b32564a8f73c81cea12468",
     "grade": false,
     "grade_id": "cell-3994e5b1f6dd1986",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 4  [marks 1]\n",
    "\n",
    "Make a function `classification_error(targets, preds)` to compute the fraction of times that the entries in `targets` do not agree with the corresponding entries in `preds`.\n",
    "\n",
    "**Note:** do not use library functions to compute the result directly but implement your own version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.627894Z",
     "start_time": "2023-02-24T12:06:00.625573Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c3b697b4e5c6a79acc56b3a33c2e7ce",
     "grade": false,
     "grade_id": "cell-38283b8cd78fca31",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classification_error(targets, preds):\n",
    "    # Get number of instances\n",
    "    n_instances = len(targets)\n",
    "    \n",
    "    # Compute number of instances that are misclassified\n",
    "    n_misclassified = 0\n",
    "    for i in range(n_instances):\n",
    "        if (targets[i] != preds[i]):\n",
    "            n_misclassified += 1\n",
    "            \n",
    "    # Compute classification error\n",
    "    class_error = n_misclassified / n_instances\n",
    "    \n",
    "    return class_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.632218Z",
     "start_time": "2023-02-24T12:06:00.629428Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47f810f07baf8b3e2e83ffed54a909a9",
     "grade": true,
     "grade_id": "cell-2c4e50d0ec1e9227",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c86fbb9199fbe9ee64006cdaae0e5862",
     "grade": false,
     "grade_id": "cell-569ce08b42229bd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5  [marks 2]\n",
    "\n",
    "Make a function `regression_error(targets, preds)` to compute the mean squared error between `targets` and `preds`.\n",
    "\n",
    "$${\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(T_{i}-{ {P_{i}}})^{2}.}$$\n",
    "\n",
    "**Note:** do not use library functions to compute the result directly but implement your own version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.636250Z",
     "start_time": "2023-02-24T12:06:00.633755Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdd5b8ce3d832d1cab61410211fb0273",
     "grade": false,
     "grade_id": "cell-41a05a42202a748d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def regression_error(targets, preds):\n",
    "    n_samples = targets.shape[0]\n",
    "    \n",
    "    mse = np.sum(np.square(targets - preds)) / n_samples\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.640602Z",
     "start_time": "2023-02-24T12:06:00.637741Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "697ec2cac2d3194d4823da20999474f2",
     "grade": true,
     "grade_id": "cell-6c6a3122c790388a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30677a3966db2bf530e5b3d4c4906622",
     "grade": false,
     "grade_id": "cell-054372fdf9cd01db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 6  [marks 7]\n",
    "\n",
    "Make a function `make_bootstrap(data_matrix, targets)` to extract a bootstrapped replicate of an input dataset. \n",
    "\n",
    "The function should return the following 6 elements (in this order): `bootstrap_data_matrix, bootstrap_targets, bootstrap_sample_ids, oob_data_matrix, oob_targets, oob_samples_ids`, where:\n",
    "1. `bootstrap_data_matrix`: is a data matrix encoding the bootstrapped replicate of the data matrix; the number of instances in `bootstrap_data_matrix` is equal to the number of instances in `data_matrix`\n",
    "2. `bootstrap_targets`: is the corresponding bootstrapped replicate of the target vector\n",
    "3. `bootstrap_sample_ids`: is an array containing the instance indices of the bootstrapped replicate of the data matrix\n",
    "4. `oob_data_matrix`: is a data matrix encoding the out of bag instances \n",
    "5. `oob_targets`: is the corresponding out of bag instances of the target vector\n",
    "6. `oob_samples_ids`: is an array containing the instance indices of the out of bag instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.645061Z",
     "start_time": "2023-02-24T12:06:00.642081Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0df64ac3989e98156db1e3f9178c628",
     "grade": false,
     "grade_id": "cell-9820cf66bd156644",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_bootstrap(data_matrix, targets):\n",
    "    random_seed = 42\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    n_samples = data_matrix.shape[0]\n",
    "    \n",
    "    bootstrap_sample_ids = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    oob_sample_ids = np.setdiff1d(np.arange(n_samples), np.unique(bootstrap_sample_ids))\n",
    "    \n",
    "    bootstrap_data_matrix = data_matrix[bootstrap_sample_ids]\n",
    "    bootstrap_targets = targets[bootstrap_sample_ids]\n",
    "    \n",
    "    oob_data_matrix = data_matrix[oob_sample_ids]\n",
    "    oob_targets = targets[oob_sample_ids]\n",
    "    \n",
    "    return bootstrap_data_matrix, bootstrap_targets, bootstrap_sample_ids, oob_data_matrix, oob_targets, oob_sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.650910Z",
     "start_time": "2023-02-24T12:06:00.646533Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a587297741dc7101065d25ebb82732",
     "grade": true,
     "grade_id": "cell-d5d05e6a91c17f6e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.656626Z",
     "start_time": "2023-02-24T12:06:00.652629Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7a8cf3883784c8f626cd377a7354d1a",
     "grade": true,
     "grade_id": "cell-144c51ff934a2f3f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78b00ab687dcb2a784503b3e43b589b4",
     "grade": false,
     "grade_id": "cell-11d4d0ffcdc7e914",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 7  [marks 9]\n",
    "\n",
    "Consider the following functional blueprints `estimator = train(X_train, y_train, param)` and `test(X_test, estimator)`. A function of type `train` takes in input a data matrix `X_train` a target vector `y_train` and a single value `param` (not a list of parameters). A function of type `train` outputs an object that represent an estimator. A function of type `test` takes in input a data matrix `X_test` the fit object `estimator` and outputs the predicted targets.\n",
    "\n",
    "Using this blueprint, write the specialised train and test functions for the following classifiers and regressors (use the function signature provided in the next cell, e.g. `train_ab` for training an adaboost classifier):\n",
    "\n",
    "Classifiers:\n",
    "- a) k-nearest-neighbor: the parameter controls the number of neighbors (you may use KNeighborsClassifier from scikit) `[train_knn, test_knn]`\n",
    "- b) adaboost: the parameter controls the maximal depth of the decision tree uses as weak classifier (you may use the DecisionTreeClassifier from scikit but **you should provide your own implementation of the boosting algorithm**) `[train_ab, test_ab]`\n",
    "- c) random forest: the parameter controls the maximal depth of the tree (you may use the DecisionTreeClassifier from scikit but **you should provide your own implementation of the bagging algorithm**) `[train_rfc, test_rfc]`\n",
    "\n",
    "Regressors:\n",
    "- d) decision tree: the parameter controls the maximal depth of the tree (you may use the DecisionTreeRegressor from scikit) `[train_dt, test_dt]`\n",
    "- e) svm linear: the parameter controls the regularization constant C (you may use SVR from scikit) `[train_svm_1, test_svm]`\n",
    "- f) svm with a polynomial kernel of degree 2: the parameter controls the regularization constant C  (you may use SVR from scikit) `[train_svm_2, test_svm]`\n",
    "- g) svm with a polynomial kernel of degree 3: the parameter controls the regularization constant C  (you may use SVR from scikit) `[train_svm_3, test_svm]`\n",
    "- h) random forest: the parameter controls the maximal depth of the tree (you may use the DecisionTreeRegressor from scikit but **you should provide your own implementation of the bagging algorithm**) `[train_rf, test_rf]`\n",
    "\n",
    "For the algorithms `adaboost` and `random forest`, the size of the ensemble should be fixed to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.671354Z",
     "start_time": "2023-02-24T12:06:00.658156Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e6e8e1169241fdd83d3145216354846",
     "grade": false,
     "grade_id": "cell-df3c9bf947c1e603",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_knn(X_train, y_train, param):\n",
    "    knn = KNeighborsClassifier(n_neighbors=param)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n",
    "\n",
    "def test_knn(X_test, est):\n",
    "    y_pred = est.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_ab(X_train, y_train, param):\n",
    "    # Initialize weights\n",
    "    weights = np.ones(len(y_train)) / len(y_train)\n",
    "    \n",
    "    models = []\n",
    "    alpha = []\n",
    "    for i in range(100):\n",
    "        clf = DecisionTreeClassifier(max_depth=param)\n",
    "        clf.fit(X_train, y_train, sample_weight=weights)\n",
    "        y_pred = clf.predict(X_train)\n",
    "\n",
    "        # Compute weighted error\n",
    "        err = np.sum(weights[y_train != y_pred])\n",
    "        alpha_i = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "        # Update weights\n",
    "        weights *= np.exp(-alpha_i * y_train * y_pred)\n",
    "        weights /= np.sum(weights)\n",
    "\n",
    "        models.append(clf)\n",
    "        alpha.append(alpha_i)\n",
    "\n",
    "    return {'models': np.array(models), 'alpha': np.array(alpha)}\n",
    "\n",
    "def test_ab(X_test, models):\n",
    "    y_pred = np.zeros(X_test.shape[0])\n",
    "    for i in range(100):\n",
    "        clf = models['models'][i]\n",
    "        alpha_i = models['alpha'][i]\n",
    "        y_pred_i = clf.predict(X_test)\n",
    "        y_pred += alpha_i * y_pred_i\n",
    "\n",
    "    return np.sign(y_pred)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_rfc(X_train, y_train, param):\n",
    "    n_estimators = 100\n",
    "    trees = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        bootstrap_data, bootstrap_targets, _, oob_data, oob_targets, _, = make_bootstrap(X_train, y_train)\n",
    "        tree = DecisionTreeClassifier(max_depth=param)\n",
    "        tree.fit(bootstrap_data, bootstrap_targets)\n",
    "        trees.append(tree)\n",
    "    return np.array(trees)\n",
    "\n",
    "def test_rfc(X_test, models):\n",
    "    n_estimators = 100\n",
    "    y_pred = np.zeros((n_estimators, len(X_test)))\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        y_pred[i] = models[i].predict(X_test)\n",
    "        \n",
    "    return np.round(np.mean(y_pred, axis=0))\n",
    "\n",
    "\n",
    "# regressors\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def train_dt(X_train, y_train, param):\n",
    "    estimator = DecisionTreeRegressor(max_depth=param)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "def test_dt(X_test, est):\n",
    "    y_pred = est.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def train_svm_1(X_train, y_train, param):\n",
    "    svm = SVR(kernel='linear', C=param)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    return svm\n",
    "\n",
    "def train_svm_2(X_train, y_train, param):\n",
    "    svm = SVR(kernel=\"poly\", degree=2, C=param)\n",
    "    svm.fit(X_train, y_train)\n",
    "    return svm\n",
    "\n",
    "def train_svm_3(X_train, y_train, param):\n",
    "    svm = SVR(kernel=\"poly\", degree=3, C=param)\n",
    "    svm.fit(X_train, y_train)\n",
    "    return svm\n",
    "\n",
    "#Note: you do not need to specialise the svm test function for each degree\n",
    "def test_svm(X_test, est):\n",
    "    y_pred = est.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def train_rf(X_train, y_train, param):\n",
    "    n_estimators = 100\n",
    "    estimators = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        bootstrap_X_train, bootstrap_y_train, _, _, _, _ = make_bootstrap(X_train, y_train)\n",
    "        estimator = DecisionTreeRegressor(max_depth=param)\n",
    "        estimator.fit(bootstrap_X_train, bootstrap_y_train)\n",
    "        estimators.append(estimator)\n",
    "        \n",
    "    return np.array(estimators)\n",
    "\n",
    "def test_rf(X_test, models):\n",
    "    n_estimators = len(models)\n",
    "    y_predict = np.zeros((n_estimators, X_test.shape[0]))\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        y_predict[i] = models[i].predict(X_test)\n",
    "        \n",
    "    return np.mean(y_predict, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.683807Z",
     "start_time": "2023-02-24T12:06:00.673253Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f0b6388c0871638ea9db50fd3284ac1",
     "grade": true,
     "grade_id": "cell-80c878649e4d112c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.742882Z",
     "start_time": "2023-02-24T12:06:00.685357Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "606f8f4909fdcbf614800913a1096d2e",
     "grade": true,
     "grade_id": "cell-b2ecf67d4ffd1c9d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.874167Z",
     "start_time": "2023-02-24T12:06:00.744289Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f49a51ed8f12eec8e5dd3a2991e95343",
     "grade": true,
     "grade_id": "cell-3893cfeb1e962d39",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.879500Z",
     "start_time": "2023-02-24T12:06:00.875708Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cc543ba68662f14092ea466f5bdd180",
     "grade": true,
     "grade_id": "cell-15b2ff0fa002b72e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.910327Z",
     "start_time": "2023-02-24T12:06:00.880966Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0aed26d7a113d504d749368e80d2c46",
     "grade": true,
     "grade_id": "cell-804333e0e5c4f7f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:00.998912Z",
     "start_time": "2023-02-24T12:06:00.916757Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b3367854bd57ca6a78e450e796e35b1",
     "grade": true,
     "grade_id": "cell-869a7a5802fd6883",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35087837b2da638f5080dc89cbd9810e",
     "grade": false,
     "grade_id": "cell-a9835f8a00d85970",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "This is just a check-point, i.e. it is for you to see that you are correctly implementing all functions. \n",
    "\n",
    "Execute the following code (just execute the next cell):\n",
    "```python\n",
    "X, y = get_dataset_classification(n_samples=240, std=30, inner_std=10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3)\n",
    "param=3\n",
    "e_knn = classification_error(y_test, test_knn(X_test, train_knn(X_train, y_train, param)))\n",
    "e_rfc = classification_error(y_test, test_rfc(X_test, train_rfc(X_train, y_train, param)))\n",
    "e_ab = classification_error(y_test, test_ab(X_test, train_ab(X_train, y_train, param)))\n",
    "print(e_knn, e_rfc, e_ab)\n",
    "```\n",
    "\n",
    "and check that the classification error for \n",
    "- k-nearest-neighbor\n",
    "- random forest classifier\n",
    "- adaboost\n",
    "is around\n",
    "\n",
    "```0.16 0.19 0.16```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.182145Z",
     "start_time": "2023-02-24T12:06:01.002633Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f48b81a4ca0a079f6cc92d1597973d4",
     "grade": true,
     "grade_id": "cell-7638b5bea8a76ff4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666 0.24537037037037038 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_dataset_classification(n_samples=240, std=30, inner_std=10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3)\n",
    "param=3\n",
    "e_knn = classification_error(y_test, test_knn(X_test, train_knn(X_train, y_train, param)))\n",
    "e_rfc = classification_error(y_test, test_rfc(X_test, train_rfc(X_train, y_train, param)))\n",
    "e_ab = classification_error(y_test, test_ab(X_test, train_ab(X_train, y_train, param)))\n",
    "print(e_knn, e_rfc, e_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d6f0067ef23ab0ed718c91cf52debc8",
     "grade": false,
     "grade_id": "cell-baa423aae2ccd501",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "This is just a check-point, i.e. it is for you to see that you are correctly implementing all functions. \n",
    "\n",
    "Execute the following code (just execute the next cell):\n",
    "\n",
    "```python\n",
    "# Just run the following code, do not modify it\n",
    "X, y = get_dataset_regression(n_samples=120, std=30, inner_std=10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3)\n",
    "param=3\n",
    "e_dt = regression_error(y_test, test_dt(X_test, train_dt(X_train, y_train, param)))\n",
    "e_svm2 = regression_error(y_test, test_svm(X_test, train_svm_2(X_train, y_train, param)))\n",
    "e_svm3 = regression_error(y_test, test_svm(X_test, train_svm_3(X_train, y_train, param)))\n",
    "print(e_dt, e_svm2, e_svm3)\n",
    "```\n",
    "\n",
    "and check that the regression error for these regressors\n",
    "- decision tree\n",
    "- svm with polynomial kernel of degree 2\n",
    "- svm with polynomial kernel of degree 3\n",
    "\n",
    "is approximately comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.190507Z",
     "start_time": "2023-02-24T12:06:01.183618Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0cbccb27a5961f89759001048c874a2",
     "grade": true,
     "grade_id": "cell-fcad6b7fe731fa70",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.86585039147855 29.12356712080578 30.36857471161513\n"
     ]
    }
   ],
   "source": [
    "# Just run the following code, do not modify it\n",
    "X, y = get_dataset_regression(n_samples=120, std=30, inner_std=10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3)\n",
    "param=3\n",
    "e_dt = regression_error(y_test, test_dt(X_test, train_dt(X_train, y_train, param)))\n",
    "e_svm2 = regression_error(y_test, test_svm(X_test, train_svm_2(X_train, y_train, param)))\n",
    "e_svm3 = regression_error(y_test, test_svm(X_test, train_svm_3(X_train, y_train, param)))\n",
    "print(e_dt, e_svm2, e_svm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e14c33d723d0048f359ed8945506cdbd",
     "grade": false,
     "grade_id": "cell-87b17b427ce7adc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 8  [marks 10]\n",
    "\n",
    "Make a function `sizes, train_errors, test_errors = compute_learning_curve(train_func, test_func, param, X, y, test_size, n_steps, n_repetitions)` to compute the train and test errors as mandated in the learning curve approach.  \n",
    "\n",
    "The regressor will be trained via `train_func` on the problem `data_matrix`, `targets` with parameter `param`. The estimate will be done averaging a number of replicates equal to `n_repetitions`, i.e. the code needs to repeat the process  `n_repetitions` times (say 10) and average the error. \n",
    "\n",
    "Note that a fraction of the data as indicated by `test_size` (say 0.3 for 30%) is going to be reserved for testing purposes. The remaining amount of data can be used in the training phase. The learning curve should be computed for an amount of training material that varies from a **minimum of 2 instances** up to all the instances available for training. The paramter `n_steps` defines the number of datasets of different size that are computed; e.g. if the number of available instances for training is 100 and `n_steps=5`, then one would perform experiments with datasets of size `[  2,  26,  51,  75, 100]`.\n",
    "\n",
    "You should use the function `regression_error` to compute the error.\n",
    "\n",
    "\n",
    "**Note:** do not use library functions (e.g. `learning_curve` in scikit) to compute the result directly but implement your own version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.196363Z",
     "start_time": "2023-02-24T12:06:01.191932Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10c61c95b206b7aaae2b64f3db817f45",
     "grade": false,
     "grade_id": "cell-88fd81865bf10bcb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_learning_curve(train_func, test_func, param, X, y, test_size, n_steps, n_repetitions):\n",
    "    random_seed = 42\n",
    "    \n",
    "    train_errors = np.zeros((n_repetitions, n_steps))\n",
    "    test_errors = np.zeros((n_repetitions, n_steps))\n",
    "    sizes = np.zeros(n_steps, dtype=int)\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        size = int((i + 1) * X.shape[0] / n_steps)\n",
    "        sizes[i] = size\n",
    "        \n",
    "        # Compute train-test-split\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        n = int(test_size * X.shape[0])\n",
    "        \n",
    "        test_idxs = indices[:n]\n",
    "        train_idxs = indices[n:]\n",
    "        \n",
    "        X_train, y_train = X[train_idxs], y[train_idxs]\n",
    "        X_test, y_test = X[test_idxs], y[test_idxs]\n",
    "        \n",
    "        for j in range(n_repetitions):\n",
    "            sample_indices = np.random.choice(X_train.shape[0], size, replace=True)\n",
    "            X_train_sampled, y_train_sampled = X_train[sample_indices], y_train[sample_indices]\n",
    "            \n",
    "            trained = train_func(X_train_sampled, y_train_sampled, param)\n",
    "            \n",
    "            train_pred = test_func(X_train_sampled, trained)\n",
    "            test_pred = test_func(X_test, trained)\n",
    "            \n",
    "            train_errors[j, i] = regression_error(train_pred, y_train_sampled)\n",
    "            test_errors[j, i] = regression_error(test_pred, y_test)\n",
    "            \n",
    "    return sizes, train_errors.mean(axis=0), test_errors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.289444Z",
     "start_time": "2023-02-24T12:06:01.197693Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac0f72d8737e4c6c9f9875ce6baf7466",
     "grade": true,
     "grade_id": "cell-3eec91d7844605ce",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.314902Z",
     "start_time": "2023-02-24T12:06:01.290990Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e92b711f34546414d39205d7015488a",
     "grade": true,
     "grade_id": "cell-259a159e336b1e94",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adb5dd58dd68c45110b4af36a7a3953b",
     "grade": false,
     "grade_id": "cell-71594a187644a8b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 9  [marks 1]\n",
    "\n",
    "Make a function `plot_learning_curve(sizes, train_errors, test_errors)` to display the train and test error as a function of the size of the training set. \n",
    "\n",
    "You should get something like:\n",
    "\n",
    "<img src='plot11.png' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.319115Z",
     "start_time": "2023-02-24T12:06:01.316572Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fecfac2d1acf30ad977d24284202bb6a",
     "grade": false,
     "grade_id": "cell-86543ebb7a72f24c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(sizes, train_errors, test_errors):\n",
    "    plt.plot(sizes, train_errors, ':', color='orange', label='Training Error')\n",
    "    plt.plot(sizes, test_errors, '-', color='blue', label='Test Error')\n",
    "    \n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.468324Z",
     "start_time": "2023-02-24T12:06:01.320792Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fd29ec85e410164d492692a4d1ad956",
     "grade": true,
     "grade_id": "cell-51334d4ebb1fcea2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e231593abdc340e33b15b50dba6e256c",
     "grade": false,
     "grade_id": "cell-d996b3c06e38ec72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 10  [marks 3]\n",
    "\n",
    "Make a function `estimate_asymptotic_error(sizes, train_errors, test_errors)` that returns an estimate of the asymptotic or Bayes error, i.e. the error made in the limit of an infinitely large training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.472354Z",
     "start_time": "2023-02-24T12:06:01.469826Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32136a56cf90671c7c064646b6293aba",
     "grade": false,
     "grade_id": "cell-d218adcf35168c1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_asymptotic_error(sizes, train_errors, test_errors):\n",
    "    diff_errors = np.abs(train_errors - test_errors)\n",
    "    \n",
    "    log_diff_errors = np.log(diff_errors)\n",
    "    \n",
    "    coeffs = np.polyfit(sizes, log_diff_errors, 1)\n",
    "    \n",
    "    asymptotic_error = np.exp(coeffs[1])\n",
    "    \n",
    "    return asymptotic_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:01.477006Z",
     "start_time": "2023-02-24T12:06:01.473790Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb6e67a4914d520253fd8a36c6d2751b",
     "grade": true,
     "grade_id": "cell-e14e0ce3d70a3d1a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0352d16d3a05143ba3b238efff8ba287",
     "grade": false,
     "grade_id": "cell-a18a34da97e63be0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "This is just a check-point, i.e. it is for you to see that you are correctly implementing all functions. \n",
    "\n",
    "Execute the following code (just execute the next cell):\n",
    "\n",
    "```python\n",
    "X, y = get_dataset_regression(n_samples=800, std=30, inner_std=10)\n",
    "train_func, test_func = train_dt, test_dt\n",
    "param=5\n",
    "sizes, train_errors, test_errors = compute_learning_curve(train_func, test_func, param, X, y, test_size=.3, n_steps=10, n_repetitions=100)\n",
    "e = estimate_asymptotic_error(train_errors, test_errors)\n",
    "print('Asymptotic error: %.1f'%e)\n",
    "plot_learning_curve(sizes, train_errors, test_errors)\n",
    "```\n",
    "\n",
    "you should get something like\n",
    "\n",
    "<img src='plot12.png' width=400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:02.407260Z",
     "start_time": "2023-02-24T12:06:01.478437Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ea3b04a382685f6b386a91f4bf39b9c",
     "grade": true,
     "grade_id": "cell-89f2955b42ee5438",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asymptotic error: 7.6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = get_dataset_regression(n_samples=800, std=30, inner_std=10)\n",
    "train_func, test_func = train_dt, test_dt\n",
    "param=5\n",
    "sizes, train_errors, test_errors = compute_learning_curve(train_func, test_func, param, X, y, test_size=.3, n_steps=10, n_repetitions=100)\n",
    "e = estimate_asymptotic_error(sizes, train_errors, test_errors)\n",
    "print('Asymptotic error: %.1f'%e)\n",
    "plot_learning_curve(sizes, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cf87ab05f40fe2629df853e538b3b08",
     "grade": false,
     "grade_id": "cell-c94a2de1e13199cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 11 [marks 6]\n",
    "\n",
    "Make a function `bias2, variance = compute_bias_variance(predictions_dict, targets)` that takes in input a dictionary of lists of predictions indexed by the instance index, and the target vector. The function should compute the squared bias component of the error and the variance components of the error for each instance.\n",
    "\n",
    "As a toy example consider: `predictions_dict={0:[1,1,1], 1:[1,-1], 2:[-1,-1,-1,1]}` and `targets=[1,1,-1]`, that is, for instance with index 0 there are 3 predictions available `[1,1,1]`, instead for instance with index 1 there are only 2 predictions available `[1,-1]`, etc. In this case, you should get `bias2=[0.  , 1.  , 0.25]` and `variance=[0.  , 1.  , 0.75]`.\n",
    "\n",
    "Note that if you apply Bessel's correction and divide by the number of instances - 1 rather than by the number of instance you will obtain, for small datasets, slightly different results (e.g. `variance=[[0. , 2. , 1.]` in the example). Either solutions are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:02.413519Z",
     "start_time": "2023-02-24T12:06:02.409333Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f14b8f7fbc03243ad0c8094b27b3f441",
     "grade": false,
     "grade_id": "cell-25ae52c28952d27b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_bias_variance(predictions_dict, targets): \n",
    "    num_instances = len(targets)\n",
    "    bias2 = np.zeros(num_instances)\n",
    "    variance = np.zeros(num_instances)\n",
    "    \n",
    "    for i in range(0, num_instances):\n",
    "        if len(predictions_dict[i]) > 0:  # Prevent calculation on empty preds\n",
    "            # Compute mean prediction for this instance\n",
    "            mean_prediction = np.mean(predictions_dict[i])\n",
    "\n",
    "            # Compute square bias component of error\n",
    "            bias2[i] = (mean_prediction - targets[i])**2\n",
    "\n",
    "            # Compute variance component of error\n",
    "            variance[i] = np.mean((predictions_dict[i] - mean_prediction)**2)\n",
    "        \n",
    "    # Exclude instances with no predictions\n",
    "    bias2 = bias2[~np.isnan(bias2)]\n",
    "    variance = variance[~np.isnan(variance)]\n",
    "    \n",
    "    return bias2, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:02.423234Z",
     "start_time": "2023-02-24T12:06:02.416057Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71679d8c75e57771c54a4e371c27037d",
     "grade": true,
     "grade_id": "cell-6b57668a0b7b2754",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9282bb2d8546760f51f45360c393854c",
     "grade": false,
     "grade_id": "cell-138f82796f4cd3e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 12 [marks 10]\n",
    "\n",
    "Make a function `bias2, variance = bias_variance_decomposition(train_func, test_func, param, data_matrix, targets, n_bootstraps)` to compute the bias variance decomposition of the error of a regressor on a given problem. The regressor will be trained via `train_func` on the problem `data_matrix`, `targets` with parameter `param`. The estimate will be done using a number of replicates equal to `n_bootstraps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:02.430145Z",
     "start_time": "2023-02-24T12:06:02.425394Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41169b2313a617eb1f6e14bd3d0480ca",
     "grade": false,
     "grade_id": "cell-2e0210536c923437",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bias_variance_decomposition(train_func, test_func, param, data_matrix, targets, n_bootstraps):\n",
    "    predictions_dict = {}\n",
    "    \n",
    "    num_instances = len(targets)\n",
    "    \n",
    "    for i in range(num_instances):\n",
    "        predictions_dict[i] = []\n",
    "        \n",
    "    for i in range(n_bootstraps):\n",
    "        bootstrap_data_matrix, bootstrap_targets, _, oob_data_matrix, _, oob_sample_ids = make_bootstrap(data_matrix, targets)\n",
    "        model = train_func(bootstrap_data_matrix, bootstrap_targets, param)\n",
    "        preds = test_func(oob_data_matrix, model)\n",
    "        for j in range(len(oob_sample_ids)):\n",
    "            predictions_dict[oob_sample_ids[j]].append(preds[j])\n",
    "            \n",
    "    bias2, variance = compute_bias_variance(predictions_dict, targets)\n",
    "    \n",
    "    return bias2, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:02.488739Z",
     "start_time": "2023-02-24T12:06:02.431811Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e77444c7aefa3c05aa30aa05a76fdac2",
     "grade": true,
     "grade_id": "cell-948c1dfbb6cc0e61",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:02.795932Z",
     "start_time": "2023-02-24T12:06:02.490261Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "137e8f1880f50bcfcefde97de1b62887",
     "grade": true,
     "grade_id": "cell-6160907ac4ad0607",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3115e8eafe93f61e2e743c28d8d64200",
     "grade": false,
     "grade_id": "cell-542d93c404e385f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 13 [marks 2]\n",
    "\n",
    "Consider the following regression problem (it does not matter that the target is only 1 and -1):\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def make_iris_data():\n",
    "    X,y = load_iris(return_X_y=True)\n",
    "    X=X[:,[0,2]]\n",
    "    y[y==2]=0\n",
    "    y[y==0]=-1\n",
    "    return X,y\n",
    "```\n",
    "\n",
    "Estimate the squared bias and variance component for each instance.  \n",
    "\n",
    "Consider as regressor a linear svm and a polynomial svm with degree 3.\n",
    "\n",
    "What is the class of the instances that have the highest bias error on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:07.853795Z",
     "start_time": "2023-02-24T12:06:02.797363Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d60058a9d3f620fcc0272aa6f936bf4",
     "grade": false,
     "grade_id": "cell-f98e40bd2056b968",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4439117005853408 0.004001008453836785\n",
      "0.041678733539721444 0.2216259388460889\n"
     ]
    }
   ],
   "source": [
    "# Just run the following code, do not modify it\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def make_iris_data():\n",
    "    X,y = load_iris(return_X_y=True)\n",
    "    X=X[:,[0,2]]\n",
    "    y[y==2]=0\n",
    "    y[y==0]=-1\n",
    "    return X,y\n",
    "\n",
    "X,y = make_iris_data()\n",
    "\n",
    "bias2, variance = bias_variance_decomposition(train_svm_1, test_svm, param=2, data_matrix=X, targets=y, n_bootstraps=100)\n",
    "bias2 = np.array(bias2)\n",
    "variance = np.array(variance)\n",
    "print(np.mean(bias2[y==1]) , np.mean(bias2[y==-1]))\n",
    "bias2, variance = bias_variance_decomposition(train_svm_3, test_svm, param=2, data_matrix=X, targets=y, n_bootstraps=100)\n",
    "print(np.mean(bias2[y==1]) , np.mean(bias2[y==-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:12.475253Z",
     "start_time": "2023-02-24T12:06:07.855912Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe5477d535bb6be3fb50660c24226711",
     "grade": true,
     "grade_id": "cell-18d4b80a0509aa21",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c03bd44265437260ff3f458289e20afb",
     "grade": false,
     "grade_id": "cell-48b2824486b6c6ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 14 [marks 6]\n",
    "\n",
    "Make a function `bs,vs = compute_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps)` to compute the average squared bias error component and the average variance component of the error for each parameter setting in the vector `params`. The regressor will be trained via `train_func` on the problem `data_matrix`, `targets` with parameter `param`. The estimate will be done using a number of replicates equal to `n_bootstraps`. To be clear, the vector `bs` contains the average square bias error for each parameter in `params` and the vector `vs` contains the average variance error for each parameter in `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:12.481011Z",
     "start_time": "2023-02-24T12:06:12.477822Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2b332776a3106ad5416d03d6429bc94",
     "grade": false,
     "grade_id": "cell-a7413326b1b02cd7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps):\n",
    "    bs = []\n",
    "    vs = []\n",
    "    \n",
    "    for param in params:\n",
    "        bias2, variance = bias_variance_decomposition(train_func, test_func, param, data_matrix, targets, n_bootstraps)\n",
    "        \n",
    "        bias2_mean = np.nanmean(bias2)\n",
    "        variance_mean = np.mean(variance)\n",
    "        \n",
    "        bs.append(bias2_mean)\n",
    "        vs.append(variance_mean)\n",
    "        \n",
    "    bs = np.array(bs)\n",
    "    vs = np.array(vs)\n",
    "    \n",
    "    return bs, vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:12.633833Z",
     "start_time": "2023-02-24T12:06:12.482564Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e24fac08a9538e943b51603adb8a63d2",
     "grade": true,
     "grade_id": "cell-913efa2d181b3145",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "937d3896d362ac89591ede97564d377d",
     "grade": false,
     "grade_id": "cell-c81f66b915335b2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 15 [marks 1]\n",
    "\n",
    "Make a function `plot_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps, logscale=False)`.\n",
    "\n",
    "You should plot the individual components or the squared bias, the variance and the total error. You should allow the possibility to employ a logarithmic scale for the horizontal axis via the `logscale` flag.\n",
    "\n",
    "You should get something like:\n",
    "\n",
    "<img src='plot18.png' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:12.638590Z",
     "start_time": "2023-02-24T12:06:12.635450Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71201436650300639351e1223a091bdf",
     "grade": false,
     "grade_id": "cell-0cff4b437bbcda3f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps, logscale=False):\n",
    "    bs, vs = compute_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps)\n",
    "    total_error = bs + vs\n",
    "    \n",
    "    plt.figure(figsize=(8 , 6))\n",
    "    \n",
    "    plt.plot(params, bs, color='blue', label='bias2')\n",
    "    plt.plot(params, vs, color='orange', label='variance')\n",
    "    plt.plot(params, total_error, color='green', label='error')\n",
    "    \n",
    "    plt.xlabel('Parameter')\n",
    "    plt.ylabel('Error')\n",
    "    \n",
    "    if logscale:\n",
    "        plt.xscale('log')\n",
    "        \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:13.487517Z",
     "start_time": "2023-02-24T12:06:12.640349Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c7ed7989aac2ba0a85257b70751c3cc",
     "grade": true,
     "grade_id": "cell-ec88b44e8808fc53",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0748f21fcbb764c04321c97f8c809d64",
     "grade": false,
     "grade_id": "cell-500d63e45e73faf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 16 [marks 2]\n",
    "\n",
    "Make a function `find_best_param_with_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps)` that uses the bias variance decomposition analysis to determine which parameter among `params` achieves the smallest estimated predictive error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:13.493160Z",
     "start_time": "2023-02-24T12:06:13.489924Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b89b218199627bd69ff6821dbeafe0b9",
     "grade": false,
     "grade_id": "cell-aa28c6e5e78e4b64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_best_param_with_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps):\n",
    "    bs, vs = compute_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps)\n",
    "    \n",
    "    total_error = bs + vs\n",
    "    \n",
    "    best_param = params[np.nanargmin(total_error)]\n",
    "    \n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:13.498247Z",
     "start_time": "2023-02-24T12:06:13.495218Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c05ad10dac8dd8cbd4919f5de60d045",
     "grade": true,
     "grade_id": "cell-98ff3545d2855471",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60afff96f21c708f004b5f15f4b358a7",
     "grade": false,
     "grade_id": "cell-410b087988cb8ce1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 17 [marks 6]\n",
    "\n",
    "When you execute the following code \n",
    "```python\n",
    "X, y = get_dataset_regression(n_samples=400, std=10, inner_std=7)\n",
    "params = np.linspace(1,30,30).astype(int)\n",
    "train_func, test_func = train_dt, test_dt\n",
    "p = find_best_param_with_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps=60)\n",
    "print('Best parameter:%s'%p)\n",
    "plot_bias_variance_decomposition(train_func, test_func, params, data_matrix, targets, n_bootstraps=50, logscale=False)\n",
    "```\n",
    "\n",
    "You should get something like:\n",
    "\n",
    "<img src='plot19.png' width=400>\n",
    "\n",
    "The next unit tests will run your functions `find_best_param_with_bias_variance_decomposition` on an undisclosed dataset using as regressors:\n",
    "- decision tree\n",
    "- svm degree 3\n",
    "\n",
    "and 3 marks will be awarded for each correct optimal parameter identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:16.837352Z",
     "start_time": "2023-02-24T12:06:13.499489Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebe5ef38e27fcca0f8e6b5f288f3b087",
     "grade": true,
     "grade_id": "cell-feb1a931d76ea8c6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.186609Z",
     "start_time": "2023-02-24T12:06:16.838924Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01a69608d91fac9ad7da4d4ad7a63a3f",
     "grade": true,
     "grade_id": "cell-76df33a1a8fbb2ed",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eef86b1ef11376632ca5f130c570beac",
     "grade": false,
     "grade_id": "cell-f59fab32129c8dd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 18 [marks 5]\n",
    "\n",
    "Make a function `conf_mtx = confusion_table(targets, preds)` to output the confusion matrix as a 2 x 2 Numpy array. Rows indicate the prediction and columns the target. The cell element with index [0,0] should report the true positive count. \n",
    "\n",
    "Running the following code:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris(return_X_y=True)\n",
    "y[y==2]=0\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3)\n",
    "\n",
    "models = train_knn(X_train, y_train, param=3)\n",
    "preds = test_knn(X_test, models)\n",
    "conf_mtx = confusion_table(y_test, preds)\n",
    "print(conf_mtx)\n",
    "```\n",
    "you should obtain something similar to\n",
    "```\n",
    "[[16.  1.]\n",
    " [ 0. 28.]]\n",
    "```\n",
    "Note: the exact values can differ in your run\n",
    "\n",
    "**Note:** do not use library functions to compute the result directly but implement your own version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.192401Z",
     "start_time": "2023-02-24T12:06:19.188128Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b00fc58c41247681c010e3389ff66e9",
     "grade": false,
     "grade_id": "cell-176a75e90af658d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion_table(targets, preds):\n",
    "    tp = np.sum((targets == 1) & (preds == 1))\n",
    "    fp = np.sum((targets == 0) & (preds == 1))\n",
    "    tn = np.sum((targets == 0) & (preds == 0))\n",
    "    fn = np.sum((targets == 1) & (preds == 0))\n",
    "    \n",
    "    return np.array([[tp, fp], [fn, tn]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.197280Z",
     "start_time": "2023-02-24T12:06:19.193788Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2593cff5ca1d4cf2178e76fe87958cf6",
     "grade": true,
     "grade_id": "cell-00a88b70a57ad1df",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "569616e12d7c3327cb19ba228a618181",
     "grade": false,
     "grade_id": "cell-39022c43725d18a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 19 [marks 1]\n",
    "\n",
    "Make a function `error_from_confusion_table(confusion_table_func, targets, preds)` that takes in input the previous `confusion_table` function and returns the error, i.e. the fraction of predictions that do not agree with the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.202045Z",
     "start_time": "2023-02-24T12:06:19.199117Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2365f18f722e8ce42bd99ba99991ea56",
     "grade": false,
     "grade_id": "cell-ebe8c8b0dbd3efe5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def error_from_confusion_table(confusion_table_func, targets, preds):\n",
    "    conf_mtx = confusion_table_func(targets, preds)\n",
    "    \n",
    "    total_preds = len(targets)\n",
    "    \n",
    "    incorrect_preds = conf_mtx[0, 1] + conf_mtx[1, 0]#\n",
    "    \n",
    "    error_rate = incorrect_preds / total_preds\n",
    "    \n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.206365Z",
     "start_time": "2023-02-24T12:06:19.203814Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1000097f4d4ae8b11ee6441001833cce",
     "grade": true,
     "grade_id": "cell-535aed4f0db3a8dc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afdc7dc4601e57bac37e957cd749a3cd",
     "grade": false,
     "grade_id": "cell-265762770e3a29f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 20 [marks 12]\n",
    "\n",
    "Make a function `predictions, out_targets = cross_validation_prediction(train_func, test_func, param, data_matrix, targets, kfold)` that estimates the predictions of a classifier trained via the function `train_func` with parameter `param` on the problem `data_matrix, targets` using a k-fold cross validation strategy with the number of folds indicated by `kfold`.\n",
    "\n",
    "Since the order of the instances associated to the predictions can be different from the original order, the function is required to output also the corresponding target values in the array `out_targets` (i.e. the value in position 10 in `predictions` corresponds to the target value in position 10 in `out_targets` )\n",
    "\n",
    "**Note:** do not use library functions (such as `KFold` or `StratifiedKFold`) but implement your own version of the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.211862Z",
     "start_time": "2023-02-24T12:06:19.207529Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dc461a62adc2fb145ae329fca3cf87e",
     "grade": false,
     "grade_id": "cell-871b84ac5ae59e38",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation_prediction(train_func, test_func, param, data_matrix, targets, kfold):\n",
    "    # Shuffle data and targets in unison\n",
    "    idx = np.random.permutation(len(data_matrix))\n",
    "    data_matrix = data_matrix[idx]\n",
    "    targets = targets[idx]\n",
    "    \n",
    "    # Compute size of each fold\n",
    "    fold_size = len(data_matrix) // kfold\n",
    "    \n",
    "    # Init predictions and out_targets\n",
    "    predictions = np.zeros(len(targets))\n",
    "    out_targets = np.zeros(len(targets))\n",
    "    \n",
    "    # Perform k-fold cross validation\n",
    "    for k in range (kfold):\n",
    "        # Compute idxs for current fold\n",
    "        start_idx = k * fold_size\n",
    "        end_idx = start_idx + fold_size\n",
    "        if k == kfold - 1:\n",
    "            end_idx = len(data_matrix)\n",
    "            \n",
    "        # Split data and target arrays into train-test\n",
    "        test_data = data_matrix[start_idx:end_idx]\n",
    "        test_targets = targets[start_idx:end_idx]\n",
    "        train_data = np.concatenate([data_matrix[:start_idx], data_matrix[end_idx:]])\n",
    "        train_targets = np.concatenate([targets[:start_idx], targets[end_idx:]])\n",
    "        \n",
    "        # Train classifier on current fold\n",
    "        classifier = train_func(train_data, train_targets, param)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        fold_prediction = test_func(test_data, classifier)\n",
    "        \n",
    "        # Update predictions and out_targets\n",
    "        predictions[start_idx:end_idx] = fold_prediction\n",
    "        out_targets[start_idx:end_idx] = test_targets\n",
    "        \n",
    "    return np.array(predictions), np.array(out_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.219246Z",
     "start_time": "2023-02-24T12:06:19.213322Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cee7461170a6f5035605a7d9e6cb9573",
     "grade": true,
     "grade_id": "cell-76f31b6cacf8f5f8",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.256383Z",
     "start_time": "2023-02-24T12:06:19.220589Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d24b9a5629423fca3182ddffd0218c66",
     "grade": true,
     "grade_id": "cell-7a433e8e5fb3c0a4",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3ff8dfc86d995eeb36dbcb0e3c208a3",
     "grade": false,
     "grade_id": "cell-b6756e854969d98b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 21 [marks 5]\n",
    "\n",
    "Make a function `mean_errors = compute_errors_with_crossvalidation(train_func, test_func, params, data_matrix, targets, kfold, n_repetitions)` that returns the estimated average error for each parameter in `params`. The classifier is trained via the function `train_func` with parameters taken from `params` on the problem `data_matrix, targets` using a k-fold cross validation strategy with the number of folds indicated by `kfold`.\n",
    "The error estimate is repeated a number of times indicated in `n_repetitions`. The error should be computed using the function `error_from_confusion_table`. The output vector `mean_errors` has as many entries as there are paramters in `params`.\n",
    "\n",
    "**Note:** do not use library functions (such as `cross_val_score`) but implement your own version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.260875Z",
     "start_time": "2023-02-24T12:06:19.257719Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dba054e6a6944e090333db353aa91bda",
     "grade": false,
     "grade_id": "cell-16f6d125d1c875b3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_errors_with_crossvalidation(train_func, test_func, params, data_matrix, targets, kfold, n_repetitions):\n",
    "    mean_errors = np.zeros(len(params))\n",
    "    \n",
    "    for i, pararm in enumerate(params):\n",
    "        errors = []\n",
    "        \n",
    "        for _ in range(n_repetitions):\n",
    "            # Compute cross validation predictions and targets\n",
    "            predictions, out_targets = cross_validation_prediction(train_func, test_func, param, data_matrix, targets, kfold)\n",
    "            \n",
    "            # Compute error rate for current parameter\n",
    "            error_rate = error_from_confusion_table(confusion_table, out_targets, predictions)\n",
    "            errors.append(error_rate)\n",
    "            \n",
    "        mean_errors[i] = np.mean(errors)\n",
    "        \n",
    "    return np.array(mean_errors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.282632Z",
     "start_time": "2023-02-24T12:06:19.262214Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf0052c6a0d4c8baf1cad3006fb667c7",
     "grade": true,
     "grade_id": "cell-0fced19d33039cab",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08ee0e3c3da7efc40ce44e3893f4c218",
     "grade": false,
     "grade_id": "cell-689c2b5f45b17896",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 22 [marks 2]\n",
    "\n",
    "Make a function `find_best_param_with_crossvalidation(train_func, test_func, params, data_matrix, targets, kfold, n_repetitions)` that uses crossvalidation to determine which parameter among `params` achieves the smallest estimated predictive error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.286565Z",
     "start_time": "2023-02-24T12:06:19.283984Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1300dd7d2038c1842c1ea9eb6b10f616",
     "grade": false,
     "grade_id": "cell-3356a77d706ff595",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_best_param_with_crossvalidation(train_func, test_func, params, data_matrix, targets, kfold, n_repetitions):\n",
    "    # Compute mean errors for each param\n",
    "    mean_errors = compute_errors_with_crossvalidation(train_func, test_func, params, data_matrix, targets, kfold, n_repetitions)\n",
    "    \n",
    "    best_param_idx = np.argmin(mean_errors)\n",
    "    \n",
    "    best_param = params[best_param_idx]\n",
    "    \n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:19.460877Z",
     "start_time": "2023-02-24T12:06:19.287914Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d2dabd02e9044c53676f88bed54be24",
     "grade": true,
     "grade_id": "cell-31eb98e3acfb9b21",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c24b1906d70f46da92e8dbdc24b97579",
     "grade": false,
     "grade_id": "cell-bb44e9b62569137f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "This is just a check-point, i.e. it is for you to see that you are correctly implementing all functions. \n",
    "\n",
    "Execute the following code (just execute the next cell):\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(return_X_y=True)\n",
    "y[y==2]=0\n",
    "params = [3,5,7,9,11]\n",
    "train_func, test_func = train_knn, test_knn\n",
    "kfold = 5\n",
    "n_repetitions = 5\n",
    "best_param = find_best_param_with_crossvalidation(train_func, test_func, params, data_matrix, targets, kfold, n_repetitions)\n",
    "print(best_param)\n",
    "```\n",
    "and get a value around 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T12:06:20.475261Z",
     "start_time": "2023-02-24T12:06:19.462712Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8f34798138fe84c414010fe546ad1b9",
     "grade": true,
     "grade_id": "cell-6a856c9021ae9c19",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(return_X_y=True)\n",
    "y[y==2]=0\n",
    "params = [3,5,7,9,11]\n",
    "train_func, test_func = train_knn, test_knn\n",
    "kfold = 5\n",
    "n_repetitions = 5\n",
    "best_param = find_best_param_with_crossvalidation(train_func, test_func, params, X, y, kfold, n_repetitions)\n",
    "print(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
